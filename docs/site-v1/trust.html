<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>CoPolitic trust and identity</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="copolitic-v1.css">
  <meta name="ai-portal" content="/CoGbx/CoPolitic_Portal_v1.yaml">
</head>
<body class="page page-trust">
  <header class="site-header">
    <div class="logo">CoPolitic</div>
    <nav class="main-nav">
      <a href="index.html">Home</a>
      <a href="why-now.html">Why now</a>
      <a href="roles.html">Roles</a>
      <a href="labs.html">Labs</a>
      <a href="exemplars.html">Exemplars</a>
      <a href="transparency.html">Transparency</a>
      <a href="trust.html" aria-current="page">Trust &amp; identity</a>
    </nav>
  </header>

  <main class="content">
    <section class="hero">
      <h1>Trust, identity and safety</h1>
      <p>
        CoPolitic is an independent civics lab that experiments with guardrails for AI and power.
        This page explains how we think about identity, trust and safety for people who take part
        in our work.
      </p>
    </section>

    <section>
      <h2>Independent, but connected</h2>
      <p>
        CoPolitic is part of a wider constellation of stewardship projects that includes CoCivium
        and CoIndex. We share some underlying language and tools for trust and identity, but
        CoPolitic is not owned or controlled by CoCivium.org.
      </p>
      <p>
        When we refer to shared concepts like <em>Digital Halo</em> and <em>MeritRank</em>,
        we are borrowing them from an open specification maintained in CoIndex. We use them
        to keep our experiments compatible with other labs, without giving any one site the
        power to police everybody else.
      </p>
    </section>

    <section>
      <h2>Handles, not exposed identities</h2>
      <p>
        People participate in CoPolitic under public handles, not public legal names.
        Behind the scenes, we may keep a private Steward ID so that multiple handles
        can be linked to a single real person or organisation when they choose to verify.
      </p>
      <p>
        We try to minimise the amount of personal data we collect and store. The
        goal is to let people build trust through their behaviour and contribution,
        not through how much identifying detail they hand over.
      </p>
    </section>

    <section>
      <h2>Digital Halo and MeritRank</h2>
      <p>
        Rather than “reputation scores”, we use two simple ideas:
      </p>
      <ul>
        <li><strong>Digital Halo</strong> – a qualitative sense of how someone tends to show up over time
            (for example: exploring, aligned, exemplar).</li>
        <li><strong>MeritRank</strong> – a rough tier of contribution in this context
            (for example: starter, reliable, anchor, steward).</li>
      </ul>
      <p>
        These are based on patterns of participation in CoPolitic labs, rooms and projects
        over time. They are designed to be:
      </p>
      <ul>
        <li>coarse, not precise scores,</li>
        <li>explanatory, not punitive,</li>
        <li>portable in language, but locally governed in practice.</li>
      </ul>
    </section>

    <section>
      <h2>Cooling off without public shaming</h2>
      <p>
        When safety or integrity requires boundaries, we prefer subtle structural tools to
        public shaming. For example, an account might be:
      </p>
      <ul>
        <li><strong>quieted</strong> – still able to see activity, but paused from initiating new actions,</li>
        <li><strong>exited</strong> – no longer participating in this context.</li>
      </ul>
      <p>
        In both cases we try to frame outcomes in neutral, process-based language
        (e.g. “participation is paused under the safety policy for this lab”), and we avoid
        public “blacklists”. Steward teams handle the detailed reasoning in private,
        governed spaces.
      </p>
    </section>

    <section>
      <h2>Risks in a hybrid human–AI world</h2>
      <p>
        We work from the assumption that powerful AI systems and motivated attackers could
        potentially connect dots across multiple data sources. That is why we:
      </p>
      <ul>
        <li>collect as little personal data as possible,</li>
        <li>separate public handles from private identity records,</li>
        <li>prefer aggregated trust signals over detailed incident logs.</li>
      </ul>
      <p>
        Our aim is to let people trust one another's track records without making it easy
        for anyone – human or machine – to dox or target them.
      </p>
    </section>

    <section class="contact">
      <h2>Questions, concerns and suggestions</h2>
      <p>
        If you have questions about how trust and identity work here, or if you see ways
        to make this approach safer, please reach out:
      </p>
      <p>
        Email: <a href="mailto:contact@InSeed.com">contact@InSeed.com</a>
      </p>
      <p>
        You can also raise suggestions or concerns via the CoPolitic GitHub repository.
      </p>
    </section>
  </main>

  <footer class="site-footer">
    <p>
      This trust and identity page is part of the CoPolitic.org CoGbx face.
      It is intended for both humans and AI assistants who need to understand
      how we treat identity and trust in this lab.
    </p>
  </footer>
</body>
</html>